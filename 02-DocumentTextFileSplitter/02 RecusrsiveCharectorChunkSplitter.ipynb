{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447db719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 51.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 chunks found in ..\\data\\documents\\lower_gemini_llm_info.txt\n",
      "chunk 1: lmm is an abbreviation that stands for **large language model**.\n",
      "\n",
      "it is a type of **artificial intelligence (ai)** program that is trained on a **massive amount of text data** (hence the word \"large\"). this extensive training allows the model to:\n",
      "\n",
      "**understand\n",
      " len: 260\n",
      "-------------------\n",
      "chunk 2: **understand\n",
      "\n",
      "** human language (like text you input).\n",
      "\n",
      "**generate\n",
      "\n",
      "** human\n",
      "\n",
      "like text (like answers, articles, summaries, and code).\n",
      "\n",
      "### ðŸ”‘ key characteristics\n",
      "\n",
      "**scale and capacity:\n",
      "\n",
      "** llms are characterized by their immense size, often containing billions or even trillions of\n",
      "\n",
      "**parameters\n",
      " len: 295\n",
      "-------------------\n",
      "chunk 3: **parameters\n",
      "\n",
      "**. this size allows them to capture intricate patterns and nuances in language.\n",
      "\n",
      "**transformer architecture:\n",
      "\n",
      "** most modern llms are built on a neural network architecture called a\n",
      "\n",
      "**transformer\n",
      " len: 211\n",
      "-------------------\n",
      "chunk 4: **transformer\n",
      "\n",
      "**, which is highly effective at tracking relationships and context across long sequences of text.\n",
      "\n",
      "**core function:\n",
      "\n",
      "** at a fundamental level, an llm works by being very good at\n",
      "\n",
      "**predicting the next word\n",
      " len: 222\n",
      "-------------------\n",
      "chunk 5: **predicting the next word\n",
      "\n",
      "** in a sequence, which allows it to generate coherent and contextually relevant text.\n",
      "\n",
      "### ðŸ’¡ common applications\n",
      "\n",
      "llms power many of the advanced ai tools you hear about today. they are used for:\n",
      "\n",
      "**conversational ai/chatbots:\n",
      " len: 255\n",
      "-------------------\n",
      "chunk 6: **conversational ai/chatbots:\n",
      "\n",
      "** like the one you are interacting with now.\n",
      "\n",
      "**content generation:\n",
      "\n",
      "** drafting emails, writing articles, creating marketing copy.\n",
      "\n",
      "**summarization and translation:\n",
      "\n",
      "** quickly condensing large documents or translating languages.\n",
      "\n",
      "**code generation:\n",
      " len: 282\n",
      "-------------------\n",
      "chunk 7: **code generation:\n",
      "\n",
      "** writing or debugging computer code based on natural language instructions.\n",
      "\n",
      "**examples of llms** include models like openai's gpt series (which powers chatgpt), google's gemini and palm, anthropic's claude, and meta's llama.\n",
      "\n",
      "---\n",
      " len: 252\n",
      "-------------------\n",
      "chunk 8: ---\n",
      "\n",
      "would you like to know more about how llms are trained or see some specific examples of what they can do?\n",
      " len: 110\n",
      "-------------------\n",
      "8 chunks found in ..\\data\\documents\\orginal_gemini_llm_info.txt\n",
      "chunk 1: LMM is an abbreviation that stands for **Large Language Model**.\n",
      "\n",
      "It is a type of **Artificial Intelligence (AI)** program that is trained on a **massive amount of text data** (hence the word \"large\"). This extensive training allows the model to:\n",
      "\n",
      "**Understand\n",
      " len: 260\n",
      "-------------------\n",
      "chunk 2: **Understand\n",
      "\n",
      "** human language (like text you input).\n",
      "\n",
      "**Generate\n",
      "\n",
      "** human\n",
      "\n",
      "like text (like answers, articles, summaries, and code).\n",
      "\n",
      "### ðŸ”‘ Key Characteristics\n",
      "\n",
      "**Scale and Capacity:\n",
      "\n",
      "** LLMs are characterized by their immense size, often containing billions or even trillions of\n",
      "\n",
      "**parameters\n",
      " len: 295\n",
      "-------------------\n",
      "chunk 3: **parameters\n",
      "\n",
      "**. This size allows them to capture intricate patterns and nuances in language.\n",
      "\n",
      "**Transformer Architecture:\n",
      "\n",
      "** Most modern LLMs are built on a neural network architecture called a\n",
      "\n",
      "**Transformer\n",
      " len: 211\n",
      "-------------------\n",
      "chunk 4: **Transformer\n",
      "\n",
      "**, which is highly effective at tracking relationships and context across long sequences of text.\n",
      "\n",
      "**Core Function:\n",
      "\n",
      "** At a fundamental level, an LLM works by being very good at\n",
      "\n",
      "**predicting the next word\n",
      " len: 222\n",
      "-------------------\n",
      "chunk 5: **predicting the next word\n",
      "\n",
      "** in a sequence, which allows it to generate coherent and contextually relevant text.\n",
      "\n",
      "### ðŸ’¡ Common Applications\n",
      "\n",
      "LLMs power many of the advanced AI tools you hear about today. They are used for:\n",
      "\n",
      "**Conversational AI/Chatbots:\n",
      " len: 255\n",
      "-------------------\n",
      "chunk 6: **Conversational AI/Chatbots:\n",
      "\n",
      "** Like the one you are interacting with now.\n",
      "\n",
      "**Content Generation:\n",
      "\n",
      "** Drafting emails, writing articles, creating marketing copy.\n",
      "\n",
      "**Summarization and Translation:\n",
      "\n",
      "** Quickly condensing large documents or translating languages.\n",
      "\n",
      "**Code Generation:\n",
      " len: 282\n",
      "-------------------\n",
      "chunk 7: **Code Generation:\n",
      "\n",
      "** Writing or debugging computer code based on natural language instructions.\n",
      "\n",
      "**Examples of LLMs** include models like OpenAI's GPT series (which powers ChatGPT), Google's Gemini and PaLM, Anthropic's Claude, and Meta's Llama.\n",
      "\n",
      "---\n",
      " len: 252\n",
      "-------------------\n",
      "chunk 8: ---\n",
      "\n",
      "Would you like to know more about how LLMs are trained or see some specific examples of what they can do?\n",
      " len: 110\n",
      "-------------------\n",
      "8 chunks found in ..\\data\\documents\\upper_gemini_llm_info.txt\n",
      "chunk 1: LMM IS AN ABBREVIATION THAT STANDS FOR **LARGE LANGUAGE MODEL**.\n",
      "\n",
      "IT IS A TYPE OF **ARTIFICIAL INTELLIGENCE (AI)** PROGRAM THAT IS TRAINED ON A **MASSIVE AMOUNT OF TEXT DATA** (HENCE THE WORD \"LARGE\"). THIS EXTENSIVE TRAINING ALLOWS THE MODEL TO:\n",
      "\n",
      "**UNDERSTAND\n",
      " len: 260\n",
      "-------------------\n",
      "chunk 2: **UNDERSTAND\n",
      "\n",
      "** HUMAN LANGUAGE (LIKE TEXT YOU INPUT).\n",
      "\n",
      "**GENERATE\n",
      "\n",
      "** HUMAN\n",
      "\n",
      "LIKE TEXT (LIKE ANSWERS, ARTICLES, SUMMARIES, AND CODE).\n",
      "\n",
      "### ðŸ”‘ KEY CHARACTERISTICS\n",
      "\n",
      "**SCALE AND CAPACITY:\n",
      "\n",
      "** LLMS ARE CHARACTERIZED BY THEIR IMMENSE SIZE, OFTEN CONTAINING BILLIONS OR EVEN TRILLIONS OF\n",
      "\n",
      "**PARAMETERS\n",
      " len: 295\n",
      "-------------------\n",
      "chunk 3: **PARAMETERS\n",
      "\n",
      "**. THIS SIZE ALLOWS THEM TO CAPTURE INTRICATE PATTERNS AND NUANCES IN LANGUAGE.\n",
      "\n",
      "**TRANSFORMER ARCHITECTURE:\n",
      "\n",
      "** MOST MODERN LLMS ARE BUILT ON A NEURAL NETWORK ARCHITECTURE CALLED A\n",
      "\n",
      "**TRANSFORMER\n",
      " len: 211\n",
      "-------------------\n",
      "chunk 4: **TRANSFORMER\n",
      "\n",
      "**, WHICH IS HIGHLY EFFECTIVE AT TRACKING RELATIONSHIPS AND CONTEXT ACROSS LONG SEQUENCES OF TEXT.\n",
      "\n",
      "**CORE FUNCTION:\n",
      "\n",
      "** AT A FUNDAMENTAL LEVEL, AN LLM WORKS BY BEING VERY GOOD AT\n",
      "\n",
      "**PREDICTING THE NEXT WORD\n",
      " len: 222\n",
      "-------------------\n",
      "chunk 5: **PREDICTING THE NEXT WORD\n",
      "\n",
      "** IN A SEQUENCE, WHICH ALLOWS IT TO GENERATE COHERENT AND CONTEXTUALLY RELEVANT TEXT.\n",
      "\n",
      "### ðŸ’¡ COMMON APPLICATIONS\n",
      "\n",
      "LLMS POWER MANY OF THE ADVANCED AI TOOLS YOU HEAR ABOUT TODAY. THEY ARE USED FOR:\n",
      "\n",
      "**CONVERSATIONAL AI/CHATBOTS:\n",
      " len: 255\n",
      "-------------------\n",
      "chunk 6: **CONVERSATIONAL AI/CHATBOTS:\n",
      "\n",
      "** LIKE THE ONE YOU ARE INTERACTING WITH NOW.\n",
      "\n",
      "**CONTENT GENERATION:\n",
      "\n",
      "** DRAFTING EMAILS, WRITING ARTICLES, CREATING MARKETING COPY.\n",
      "\n",
      "**SUMMARIZATION AND TRANSLATION:\n",
      "\n",
      "** QUICKLY CONDENSING LARGE DOCUMENTS OR TRANSLATING LANGUAGES.\n",
      "\n",
      "**CODE GENERATION:\n",
      " len: 282\n",
      "-------------------\n",
      "chunk 7: **CODE GENERATION:\n",
      "\n",
      "** WRITING OR DEBUGGING COMPUTER CODE BASED ON NATURAL LANGUAGE INSTRUCTIONS.\n",
      "\n",
      "**EXAMPLES OF LLMS** INCLUDE MODELS LIKE OPENAI'S GPT SERIES (WHICH POWERS CHATGPT), GOOGLE'S GEMINI AND PALM, ANTHROPIC'S CLAUDE, AND META'S LLAMA.\n",
      "\n",
      "---\n",
      " len: 252\n",
      "-------------------\n",
      "chunk 8: ---\n",
      "\n",
      "WOULD YOU LIKE TO KNOW MORE ABOUT HOW LLMS ARE TRAINED OR SEE SOME SPECIFIC EXAMPLES OF WHAT THEY CAN DO?\n",
      " len: 110\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../data/documents\")\n",
    "\n",
    "dir_loader = DirectoryLoader(path, \n",
    "                             glob = \"**/*.txt\", \n",
    "                             loader_kwargs = {'encoding': 'utf8'}, \n",
    "                             show_progress = True)\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\"],\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 80,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "docs = dir_loader.load()\n",
    "\n",
    "for item in docs:\n",
    "    if item.metadata['source'].endswith('.txt'):\n",
    "        chunks = rec_text_splitter.split_text(item.page_content)\n",
    "        print(f\"{len(chunks)} chunks found in {item.metadata['source']}\")\n",
    "        for i,item in enumerate(chunks):\n",
    "            print(f\"chunk {i+1}: {item}\")\n",
    "            print(f\" len: {len(item)}\")\n",
    "            print('-------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
