{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceedcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a frustrating situation. While East US is generally one of the regions that supports AI features, there are several common requirements and settings that can still prevent you from creating an AI Agent (also referred to as Fabric Data Agent).\n",
      "---------------------------\n",
      "Here are the most likely reasons and steps to check:\n",
      "---------------------------\n",
      "1. Check Your Capacity Tier (This is Critical)\n",
      "---------------------------\n",
      "The AI Agent feature requires a paid Microsoft Fabric capacity.\n",
      "---------------------------\n",
      "Requirement: Your workspace must be assigned to a paid Microsoft Fabric capacity (F2 or higher) or a Power BI Premium capacity (P1 or higher) with Fabric features enabled.\n",
      "---------------------------\n",
      "Action:\n",
      "---------------------------\n",
      "Go to your Workspace settings.\n",
      "---------------------------\n",
      "Check the assigned Capacity. A free/trial capacity usually will not work for this specific feature. You need at least an F2 SKU.\n",
      "---------------------------\n",
      "2. Verify Tenant Admin Settings (Requires Admin Access)\n",
      "---------------------------\n",
      "Even if the capacity is in the right region, the feature can be disabled at the tenant level. You will need a Fabric or Global Admin to check these settings:\n",
      "---------------------------\n",
      "Go to the Admin portal (Settings gear icon ⚙ > Admin portal).\n",
      "---------------------------\n",
      "Navigate to Tenant settings.\n",
      "---------------------------\n",
      "Look for the following settings and ensure they are Enabled for the organization or a security group that includes you:\n",
      "---------------------------\n",
      "Users can create Fabric items (general Fabric switch).\n",
      "---------------------------\n",
      "Users can use Copilot and Azure OpenAI (main AI switch).\n",
      "---------------------------\n",
      "Data agent (The specific setting for creating AI Agents).\n",
      "---------------------------\n",
      "Allow data sent to Azure OpenAI to be processed outside of the capacity's geographic region (Crucial if your capacity region is not one of the few regions where the underlying Azure OpenAI Service runs).\n",
      "---------------------------\n",
      "3. Confirm the Capacity Region vs. Home Region\n",
      "---------------------------\n",
      "While East US is a supported region, for some AI features like Copilot, Microsoft historically had a requirement that the Capacity Region must match the Tenant Home Region unless the Cross-Geo Processing setting (mentioned above) is enabled.\n",
      "---------------------------\n",
      "Action: Ensure both your Tenant Home Region (found in Help > About Microsoft Fabric) and your Capacity Region (found in Admin Portal > Capacity settings) are in East US. If they differ, make sure your admin has enabled the Cross-Geo Processing setting.\n",
      "---------------------------\n",
      "4. Wait for Feature Rollout\n",
      "---------------------------\n",
      "The AI Agent feature is still in public preview and rollouts can be gradual, even within a supported region like East US.\n",
      "---------------------------\n",
      "Action: If all other requirements are met, try again after waiting 12-24 hours, as capacity settings sometimes take time to propagate fully. You can also try clearing your browser cache.\n",
      "---------------------------\n",
      "If all checks above pass and you still can't create the AI Agent, there may be a temporary resource constraint or an intermittent issue with the underlying Azure AI services in that specific datacenter.\n",
      "---------------------------\n",
      "11. What is a Shared Access Signature (SAS)? When would you use it?\n",
      "---------------------------\n",
      "A Shared Access Signature (SAS) is a URI that grants secure, delegated, and restricted access to resources in your Azure Storage account.1 It is a token that is appended to a resource URI, allowing a client to access the resource with a specific set of permissions and for a limited period of time, without having to expose your storage account's master access keys.2\n",
      "---------------------------\n",
      "You would use a SAS in scenarios where you need to provide limited and temporary access to a client that you don't fully trust with your master account keys.3 This is common in the following situations:\n",
      "---------------------------\n",
      "Delegating Access to Clients: A common pattern is to have a lightweight service authenticate a client and then generate a SAS token.4 The client can then use this token to access Azure Storage resources directly, without the need for all data to be routed through your own service.5 This is especially useful for applications that involve high-volume data uploads or downloads.6\n",
      "---------------------------\n",
      "Secure Data Sharing: When you need to share a file, folder, or container with an external user or application for a limited time.7 For example, a media company could use a SAS to provide temporary access to a video file for a partner to download.\n",
      "---------------------------\n",
      "In Copy Operations: A SAS is often required to authorize access to a source object when copying a blob or file between different storage accounts.8\n",
      "---------------------------\n",
      "12. What are the different types of SAS?\n",
      "---------------------------\n",
      "There are three main types of Shared Access Signatures in Azure Storage, each with different use cases and security models:9\n",
      "---------------------------\n",
      "User Delegation SAS:\n",
      "---------------------------\n",
      "Security: This is the most secure and recommended type of SAS. It is signed with Microsoft Entra ID credentials (formerly Azure Active Directory), which provides superior security compared to using the storage account key.10\n",
      "---------------------------\n",
      "Scope: Applies to Blob storage and Data Lake Storage Gen2 only.11 It can grant access at the container, directory, or blob level.12\n",
      "---------------------------\n",
      "Use Case: This is the preferred method for granting limited access to individual users or applications that can be authenticated with Microsoft Entra ID.13 It allows for fine-grained control and is easier to audit.14\n",
      "---------------------------\n",
      "Service SAS:\n",
      "---------------------------\n",
      "Security: This SAS is signed with the storage account key.15\n",
      "---------------------------\n",
      "Scope: Delegates access to a resource in a single storage service, such as Blob Storage, Queue Storage, Table Storage, or Azure Files.16\n",
      "---------------------------\n",
      "Use Case: Ideal for scenarios where you need to grant limited access to a specific resource (e.g., a single blob or a specific queue) to a client that cannot be authenticated via Microsoft Entra ID.\n",
      "---------------------------\n",
      "Account SAS:\n",
      "---------------------------\n",
      "Security: This SAS is also signed with the storage account key.17\n",
      "---------------------------\n",
      "Scope: Provides broad access to resources in one or more of the storage services within the entire storage account.18\n",
      "---------------------------\n",
      "Use Case: Suitable for applications that need to perform operations across multiple services or resources that are not available with a Service SAS, such as listing all blob containers in the account.19 Because of its broad permissions, it should be used with extreme caution.\n",
      "---------------------------\n",
      "13. What are the security best practices when using SAS tokens?\n",
      "---------------------------\n",
      "Using SAS tokens securely is critical to protecting your Azure Storage resources. Microsoft and the big data community recommend the following best practices:\n",
      "---------------------------\n",
      "Use a User Delegation SAS when possible: This is the most secure option because it's signed with Microsoft Entra ID credentials instead of the account key, reducing the risk of key compromise.20\n",
      "---------------------------\n",
      "Use a Stored Access Policy for Service SAS: For Service SAS, you can define a stored access policy on a container or file share.21 This provides a central point of control to easily manage, update, and revoke multiple SAS tokens that use the same policy.22 If you need to revoke access, you can simply delete or modify the policy.23\n",
      "---------------------------\n",
      "Use HTTPS: Always use HTTPS to transmit or distribute a SAS token.24 If a SAS token is passed over unencrypted HTTP, a malicious user could intercept it and use it.25\n",
      "---------------------------\n",
      "Minimize the SAS scope (Principle of Least Privilege):\n",
      "---------------------------\n",
      "Restrict Permissions: Only grant the minimum permissions required for the task (e.g., read-only access instead of read, write, and delete).26\n",
      "---------------------------\n",
      "Shorten Expiry Time: Set the expiration time for the SAS as short as possible.27 If a token is compromised, the window for a malicious actor to use it will be very small.\n",
      "---------------------------\n",
      "Limit Resource Scope: Grant access to a specific file or container rather than the entire storage account.28\n",
      "---------------------------\n",
      "Monitor and Log: Enable Azure Storage logging and monitor for unusual activity related to SAS tokens.29 This helps you detect and investigate potential misuse or unauthorized access.30\n",
      "---------------------------\n",
      "Plan for Revocation: Have a plan for revoking a compromised SAS.31 For tokens signed with a stored access policy, revocation is simple. For Service SAS and Account SAS that are not tied to a policy, the only way to revoke them is to regenerate the storage account keys, which will invalidate all SAS tokens generated with the old keys and can be disruptive to applications.32\n",
      "---------------------------\n",
      "Do not embed SAS in public applications: A SAS should never be hard-coded into an application, especially a public one, as it can be easily extracted and misused.33 The SAS should be generated on the fly by a secure backend service.\n",
      "---------------------------\n",
      "This is a common and critical architecture question when implementing Databricks Unity Catalog. The complete architecture involves the following layers:\n",
      "---------------------------\n",
      "Cloud Infrastructure (Physical Isolation): Storage Accounts, Containers, and Access Credentials.\n",
      "---------------------------\n",
      "Unity Catalog (Logical Governance): Metastore, Catalogs, Schemas, and Access Rules.\n",
      "---------------------------\n",
      "Compute/Workspaces (Execution Isolation): Databricks Workspaces.\n",
      "---------------------------\n",
      "Here is the complete recommended architecture for a robust Dev, UAT, and Prod environment using Unity Catalog.\n",
      "---------------------------\n",
      "Complete Unity Catalog Architecture for Dev, UAT, and Prod\n",
      "---------------------------\n",
      "1. Cloud Storage Layer (Physical Isolation)\n",
      "---------------------------\n",
      "The best practice is to physically isolate the managed data for each environment.\n",
      "---------------------------\n",
      "Component Recommendation Example Path (AWS S3/Azure ADLS) Rationale Storage Account Use one dedicated Storage Account for all Unity Catalog managed data. (If cost attribution or performance limits are a major concern, you may use separate SAs, but one is often sufficient). my-uc-data-storage-account Centralized management; separate SAs add cost/complexity. Metastore Location Do not set a default storage location at the Metastore level. If you must set one (for legacy reasons), it should be a dedicated, non-data container. N/A (or a dedicated folder for metadata) Avoids mixing all data in one place; data isolation starts at the Catalog. Environment Containers Create separate containers or top-level folders for each environment's managed data. abfss://dev-container@<SA>/ abfss://uat-container@<SA>/ abfss://prod-container@<SA>/ Critical Isolation: Physically separates Dev, UAT, and Prod data for security, cleanup, and compliance. Storage Credentials Use one Storage Credential per\n",
      "---------------------------\n",
      "cleanup, and compliance. Storage Credentials Use one Storage Credential per environment (or per storage account/container) to map Databricks to the cloud storage. sc_dev, sc_uat, sc_prod Granular control over the identity UC uses to read/write data in each environment's container. External Locations Create External Locations that use the Storage Credentials to point to the environment's storage path. el_dev → sc_dev → abfss://dev-container@.../ A required UC securable object to bind the storage to the catalog.\n",
      "---------------------------\n",
      "2. Unity Catalog Layer (Logical Governance)\n",
      "---------------------------\n",
      "This is the core of the governance model.\n",
      "---------------------------\n",
      "Component Recommendation Example Name Purpose Metastore One Metastore per region. us_east_metastore The single point of control for all data assets and permissions in that region. Catalogs Catalog per Environment. dev_catalog, uat_catalog, prod_catalog The Primary Unit of Isolation: Permissions, data, and access are managed at this level. This is where you specify the Managed Location. Schemas (Databases) Schema per Data Layer (Bronze, Silver, Gold) or per Business Domain. prod_catalog.gold dev_catalog.bronze Logical organization within the environment (e.g., separating raw from transformed data).\n",
      "---------------------------\n",
      "3. Execution Layer (Databricks Workspaces)\n",
      "---------------------------\n",
      "The workspace-level design can vary, but strong isolation is recommended.\n",
      "---------------------------\n",
      "Component Recommendation Rationale Workspaces Dedicated Workspace per Environment is the safest and most common best practice for separation. Prevents a Dev user from running an accidental query on a production cluster, even with proper UC permissions. Catalog Binding Bind the respective catalog to its environment workspace. prod_catalog is the default catalog for the Prod workspace. Cross-Environment Read Access Bind the prod_catalog as READ-ONLY to the uat_workspace and dev_workspace. Allows developers and testers to safely reference production data without any ability to modify it. User Access Permissions are managed at the Catalog level, not the Workspace level. Grant SELECT on prod_catalog to the \"Data Scientists\" group, but only grant CREATE TABLE and MODIFY on dev_catalog to the \"Data Engineers\" group.\n",
      "---------------------------\n",
      "Architectural Flow (Summary)\n",
      "---------------------------\n",
      "The Databricks Account Admin creates the Metastore.\n",
      "---------------------------\n",
      "The Metastore Admin (or data platform team) configures the Storage Credentials and External Locations for /dev/, /uat/, and /prod/ storage paths.1\n",
      "---------------------------\n",
      "The Metastore Admin creates the three Catalogs (dev_catalog, uat_catalog, prod_catalog) and assigns the dedicated Managed Location (via the External Location) to each one.2\n",
      "---------------------------\n",
      "A developer creates a managed table: CREATE TABLE dev_catalog.gold.my_new_table AS SELECT...\n",
      "---------------------------\n",
      "Unity Catalog automatically knows:\n",
      "---------------------------\n",
      "Where to store the data (in the dedicated dev-container storage).\n",
      "---------------------------\n",
      "Which users can access it (based on permissions granted on dev_catalog).\n",
      "---------------------------\n",
      "What to do when the table is dropped (automatically clean up the data files in the dev-container).\n",
      "---------------------------\n",
      "That's an excellent follow-up question. The amount of time it will take you to cover this path and achieve job-ready proficiency in Agentic AI depends heavily on your existing Data Engineering experience and the intensity of your learning schedule.\n",
      "---------------------------\n",
      "Here is a realistic timeframe broken down by the phases, assuming you dedicate approximately 10-15 hours per week to focused learning and hands-on projects.\n",
      "---------------------------\n",
      "⏳ Estimated Transition Timeline\n",
      "---------------------------\n",
      "Phase Focus Area Estimated Time Key Goal & Deliverable Phase 1: Foundation (Data Layer) Vector DBs (RAG), Data Quality 2 - 4 Weeks Goal: Architect a RAG pipeline from scratch (e.g., ingest documents, embed them, store in a VDB). Phase 2: Core Engine (LLM) Prompt Engineering, GenAI Concepts 3 - 5 Weeks Goal: Master advanced prompting techniques (CoT, ReAct) and deploy a simple, custom LLM endpoint. Phase 3: Orchestration & Architecture LangChain/AutoGen/CrewAI, Tool Use 6 - 10 Weeks Goal: Build a functional Multi-Agent System where agents collaborate to solve a business problem (e.g., an agent that researches a topic, and a second agent that uses an API to format the result). Phase 4: AgentOps & Production Monitoring, Guardrails, Reliability 4 - 8 Weeks Goal: Operationalize your agent system, adding robust logging, error handling, and security measures suitable for a production environment. TOTAL ESTIMATE (For Job-Ready Proficiency) 4 - 7 Months You are now an AI Systems Orchestration\n",
      "---------------------------\n",
      "Job-Ready Proficiency) 4 - 7 Months You are now an AI Systems Orchestration Engineer. You can design, build, and deploy a reliable, production-grade agentic workflow.\n",
      "---------------------------\n",
      "Key Factors Influencing Your Timeframe\n",
      "---------------------------\n",
      "Your Data Engineering Level (The Accelerator):\n",
      "---------------------------\n",
      "High Proficiency in MLOps/Cloud: If you already manage production ML pipelines, you'll grasp Phase 4 (AgentOps) much faster, as it's an extension of MLOps. This can shave 1-2 months off the total time.\n",
      "---------------------------\n",
      "Experience with Python/APIs: Your experience building pipelines means Phase 3 (Tool Integration) will be intuitive, as integrating an agent with an API is similar to integrating a data source.\n",
      "---------------------------\n",
      "Focus on Practical Projects (The Critical Path):\n",
      "---------------------------\n",
      "Theory is Fast, Practice is Slow: You can cover the theory of LangChain in a weekend, but the skill is gained by debugging why your multi-agent system is failing to correctly call an API or handle a hallucination. The bulk of the time is spent on hands-on coding and debugging.\n",
      "---------------------------\n",
      "The Newness of the Field (The Uncertainty Factor):\n",
      "---------------------------\n",
      "Agentic AI frameworks (LangChain, AutoGen, CrewAI) are constantly changing. You will spend time learning new versions, debugging breaking changes, and sifting through documentation. This is normal in a new field and should be factored into the time.\n",
      "---------------------------\n",
      "Advice for the Fastest Path\n",
      "---------------------------\n",
      "Focus on the \"Workflow\": As a data engineer, your strongest unique selling point is the ability to build reliable workflows. Focus heavily on Phase 3 and 4. A recruiter will be more interested in an engineer who can deploy a reliable multi-agent system than a data scientist who can only build the underlying LLM model.\n",
      "---------------------------\n",
      "The \"Agentic Data Engineer\" Niche: Look for projects that directly apply agents to data engineering challenges:\n",
      "---------------------------\n",
      "An agent that self-heals a broken ETL pipeline.\n",
      "---------------------------\n",
      "An agent that writes dbt/SQL code based on natural language requirements.\n",
      "---------------------------\n",
      "An agent that performs intelligent data quality monitoring (notifying you only when the anomaly matters).\n",
      "---------------------------\n",
      "By concentrating your learning on the integration, reliability, and orchestration aspects, you can leverage your existing data engineering expertise to become proficient in Agentic AI very efficiently.\n",
      "---------------------------\n",
      "Would you like me to suggest a starter project that incorporates Phases 1, 2, and 3 to help you begin your hands-on journey?\n",
      "---------------------------\n",
      "Yes, you can design a workflow for user access management in a Microsoft Fabric Power BI reporting environment using Agentic AI. While standard Fabric and Power BI tools handle most access management tasks, an agentic AI workflow could automate and streamline complex or dynamic scenarios. This is achieved by integrating a Fabric data agent with an external AI platform like Azure AI Foundry Agent Service.\n",
      "---------------------------\n",
      "Here's how such a workflow could be designed:\n",
      "---------------------------\n",
      "1. The Request Agent\n",
      "---------------------------\n",
      "This agent acts as the primary interface for users.\n",
      "---------------------------\n",
      "A user submits a request for access to a specific Power BI report or workspace through a natural language interface (e.g., a chatbot or a custom Teams application).\n",
      "---------------------------\n",
      "The agent parses the request, identifies the user, and the specific item they're requesting access to.\n",
      "---------------------------\n",
      "2. The Validation Agent\n",
      "---------------------------\n",
      "The request is then passed to a validation agent.\n",
      "---------------------------\n",
      "This agent uses a Fabric data agent to access and query the organization's user and permission data, which might be stored in a Lakehouse or Warehouse.\n",
      "---------------------------\n",
      "It checks against established rules, such as the user's role, department, or existing security group memberships.\n",
      "---------------------------\n",
      "This agent can also perform additional checks, for example, to see if the user has completed mandatory security training.\n",
      "---------------------------\n",
      "3. The Approval Agent\n",
      "---------------------------\n",
      "If the request passes the initial validation, the approval agent sends a notification to the appropriate manager or data owner for approval.\n",
      "---------------------------\n",
      "This agent can be designed to understand the organizational hierarchy and automatically route the request to the correct person. It can also use Natural Language Generation (NLG) to create a concise summary of the request.\n",
      "---------------------------\n",
      "The manager can approve or deny the request directly from a notification (e.g., in Teams or Outlook).\n",
      "---------------------------\n",
      "4. The Execution Agent\n",
      "---------------------------\n",
      "Once approved, the execution agent takes over.\n",
      "---------------------------\n",
      "This agent is responsible for making the actual changes. It can call the necessary Microsoft Fabric or Power BI APIs to assign the user the appropriate workspace role (e.g., Viewer) or grant item permissions to the specific report.\n",
      "---------------------------\n",
      "It can also apply row-level security (RLS) by adding the user to a specific RLS role, ensuring they only see the data they're permitted to view.\n",
      "---------------------------\n",
      "Benefits of this Agentic Workflow\n",
      "---------------------------\n",
      "Speed and Efficiency: The process is automated end-to-end, reducing the time it takes to grant access from days to minutes.\n",
      "---------------------------\n",
      "Reduced Human Error: Agents perform checks and execute changes programmatically, eliminating manual mistakes.\n",
      "---------------------------\n",
      "Improved Governance: The workflow ensures that every access request is validated against policy and properly documented.\n",
      "---------------------------\n",
      "Scalability: The system can handle a large volume of requests without increasing the workload on IT or data teams.\n",
      "---------------------------\n",
      "This workflow leverages the strengths of Fabric's unified data estate for a holistic view of user and data permissions, while utilizing the reasoning and execution capabilities of a multi-agent system built on a platform like Azure AI Foundry.\n",
      "---------------------------\n",
      "You can learn more about how agents can be used with Fabric databases, Power BI, and other tools in this video about agentic AI apps.\n",
      "---------------------------\n",
      "<br/>\n",
      "---------------------------\n",
      "Enable Agentic AI Apps with a Unified Data Estate in Microsoft Fabric\n",
      "---------------------------\n",
      "This video demonstrates how agentic AI applications, powered by Azure AI Foundry, can integrate with Microsoft Fabric to provide unified data solutions.\n",
      "---------------------------\n",
      "AI (Artificial Intelligence), particularly through the discipline of AIOps (AI for IT Operations), is transforming IT Infrastructure management and the DevOps lifecycle by introducing advanced automation, predictive analytics, and intelligent decision-making.\n",
      "---------------------------\n",
      "Here are the key use cases across both Infra and DevOps:\n",
      "---------------------------\n",
      "I. Infrastructure & Operations (AIOps) Use Cases\n",
      "---------------------------\n",
      "These focus on managing and optimizing the production environment and underlying infrastructure.\n",
      "---------------------------\n",
      "Use Case Description Key Benefits 1. Predictive Maintenance & Anomaly Detection AI models analyze historical performance data, logs, and metrics to learn \"normal\" behavior. They detect subtle deviations (anomalies) and predict potential failures (e.g., hardware failure, resource exhaustion, or system crashes) before they impact users. Prevents outages, minimizes downtime, and shifts from reactive troubleshooting to proactive maintenance. 2. Intelligent Root Cause Analysis (RCA) AI aggregates and correlates massive amounts of data from logs, metrics, traces, and events across different tools (reducing \"alert fatigue\"). It then pinpoints the specific root cause of an incident, even in complex, distributed environments. Significantly faster Mean Time To Resolution (MTTR) and better operational efficiency. 3. Automated Incident Response & Remediation When an issue is detected, the AI can automatically trigger and execute pre-defined remediation workflows, such as restarting a service,\n",
      "---------------------------\n",
      "and execute pre-defined remediation workflows, such as restarting a service, rolling back a recent deployment, or scaling up resources, without human intervention. Faster resolution of common issues, freeing up engineers for complex problems. 4. Cloud Cost & Resource Optimization AI analyzes infrastructure usage patterns, application performance, and cost data in real-time. It provides recommendations for, or automatically implements, resource adjustments (scaling down idle resources, rightsizing VMs) for optimal efficiency. Reduces cloud spending and improves Resource Utilization and ROI. 5. Capacity Planning AI predicts future resource needs (CPU, memory, storage) based on historical trends, seasonality, and projected growth, allowing teams to proactively provision capacity. Prevents performance bottlenecks and ensures system stability during peak load. 6. Enhanced Security Operations (SecOps) AI models establish baselines for normal user and system behavior. They monitor for and\n",
      "---------------------------\n",
      "establish baselines for normal user and system behavior. They monitor for and detect suspicious activity, zero-day threats, and sophisticated attacks much faster and more accurately than traditional rule-based systems. Proactive threat detection and continuous security compliance.\n",
      "---------------------------\n",
      "II. DevOps & Development Use Cases\n",
      "---------------------------\n",
      "These focus on integrating AI into the software delivery pipeline (CI/CD) to improve speed and quality.\n",
      "---------------------------\n",
      "Use Case Description Key Benefits 1. AI-Powered Code Generation & Assistance Generative AI models (like Large Language Models) provide real-time code suggestions, autocomplete functions, and generate unit test skeletons, documentation, and boilerplate code (e.g., using tools like GitHub Copilot). Dramatically accelerates development speed and improves code consistency. 2. Intelligent Testing & Quality Assurance AI can automatically generate, optimize, and prioritize test cases based on code changes, risk profiles, and user behavior. It can also detect visual regressions or performance anomalies during the testing phase. Higher test coverage, earlier bug detection, and faster CI/CD pipeline runs. 3. CI/CD Pipeline Optimization AI analyzes build and deployment histories to predict the success of a new build, prioritize which tests to run first, and automatically suggest or implement optimizations to the pipeline configuration. Reduced deployment failures, optimized build times, and more\n",
      "---------------------------\n",
      "configuration. Reduced deployment failures, optimized build times, and more stable releases. 4. Automated Code Review & Security Scanning (DevSecOps) AI can review code for stylistic issues, performance bottlenecks, and security vulnerabilities (like SQL injection or XSS) more thoroughly and consistently than human reviewers. Improves code quality, enforces standards, and integrates security checks seamlessly into the development process. 5. Documentation & Knowledge Management AI can automatically generate post-mortem summaries, create deployment documentation, and organize internal knowledge bases, making it easier for teams to share information and onboard new members. Reduces manual administrative work and combats knowledge silos.\n",
      "---------------------------\n",
      "Based on a review of numerous online resources, here is a compilation of over 100 interview questions covering Power BI, Power Automate, and their integration.\n",
      "---------------------------\n",
      "Foundational Concepts\n",
      "---------------------------\n",
      "What is Power BI?\n",
      "---------------------------\n",
      "What is Power Automate?\n",
      "---------------------------\n",
      "What is the Power Platform?\n",
      "---------------------------\n",
      "What are the core components of Power BI? (Power Query, Power Pivot, Power View, Power Map)\n",
      "---------------------------\n",
      "What are the main components of a Power Automate flow? (Triggers, Actions, Connectors)\n",
      "---------------------------\n",
      "How do Power BI and Power Automate complement each other?\n",
      "---------------------------\n",
      "Explain the different types of flows in Power Automate. (Automated, Instant, Scheduled, Business Process, Desktop)\n",
      "---------------------------\n",
      "What are connectors in Power Automate, and why are they important?\n",
      "---------------------------\n",
      "Explain the concept of dynamic content in Power Automate.\n",
      "---------------------------\n",
      "What is a \"Data Loss Prevention (DLP) policy\" in Power Automate?\n",
      "---------------------------\n",
      "How do you handle errors in a Power Automate flow?\n",
      "---------------------------\n",
      "What is the difference between a standard and a premium connector?\n",
      "---------------------------\n",
      "How would you monitor the run history of a flow?\n",
      "---------------------------\n",
      "What are expressions in Power Automate?\n",
      "---------------------------\n",
      "What are variables in Power Automate?\n",
      "---------------------------\n",
      "Power BI & Power Automate Integration\n",
      "---------------------------\n",
      "How can you trigger a Power Automate flow directly from a Power BI report?\n",
      "---------------------------\n",
      "Describe a real-world scenario where you would use the Power Automate visual in Power BI.\n",
      "---------------------------\n",
      "What are the key steps to set up the Power Automate visual in Power BI?\n",
      "---------------------------\n",
      "How do you pass data from a Power BI report to a Power Automate flow?\n",
      "---------------------------\n",
      "Can you send an email notification from a Power BI report? If so, how?\n",
      "---------------------------\n",
      "Explain how to automate the refresh of a Power BI dataset using Power Automate.\n",
      "---------------------------\n",
      "How would you use Power Automate to export a Power BI report as a PDF and email it?\n",
      "---------------------------\n",
      "What are some of the limitations of the Power Automate visual in Power BI?\n",
      "---------------------------\n",
      "How can you automate data-driven alerts from Power BI?\n",
      "---------------------------\n",
      "Describe a scenario where you'd use Power Automate to write back data from a Power BI report.\n",
      "---------------------------\n",
      "How do you handle Row-Level Security (RLS) when integrating Power BI with Power Automate?\n",
      "---------------------------\n",
      "Can you use Power Automate to manage Power BI user access or workspace permissions?\n",
      "---------------------------\n",
      "Explain how to get data from Power BI to a SharePoint list using Power Automate.\n",
      "---------------------------\n",
      "How would you use Power Automate to post a message to a Teams channel whenever a certain KPI in a Power BI report drops below a threshold?\n",
      "---------------------------\n",
      "What's the process for setting up a data alert in Power BI and then triggering a flow?\n",
      "---------------------------\n",
      "Power BI-Specific Questions\n",
      "---------------------------\n",
      "What are the main components of Power BI Desktop?\n",
      "---------------------------\n",
      "Explain the difference between Import mode and DirectQuery mode.\n",
      "---------------------------\n",
      "What is DAX?\n",
      "---------------------------\n",
      "What is the difference between a Calculated Column and a Measure?\n",
      "---------------------------\n",
      "How do you handle data relationships in Power BI?\n",
      "---------------------------\n",
      "What is Row-Level Security (RLS)?\n",
      "---------------------------\n",
      "Explain the purpose of the CALCULATE function in DAX.\n",
      "---------------------------\n",
      "What is Power Query and the M language?\n",
      "---------------------------\n",
      "Describe the different types of filters in Power BI.\n",
      "---------------------------\n",
      "How would you optimize the performance of a Power BI report?\n",
      "---------------------------\n",
      "What is a Power BI Dashboard versus a Report?\n",
      "---------------------------\n",
      "How do you publish a report to the Power BI Service?\n",
      "---------------------------\n",
      "What is a Power BI Gateway, and why is it needed?\n",
      "---------------------------\n",
      "Explain how to create a custom visual in Power BI.\n",
      "---------------------------\n",
      "What are bookmarks and buttons in Power BI?\n",
      "---------------------------\n",
      "How do you connect to various data sources in Power BI?\n",
      "---------------------------\n",
      "What is Query Folding?\n",
      "---------------------------\n",
      "Explain the difference between Page-Level and Report-Level filters.\n",
      "---------------------------\n",
      "What are Dataflows in Power BI?\n",
      "---------------------------\n",
      "What is a KPI in Power BI?\n",
      "---------------------------\n",
      "Power Automate-Specific Questions\n",
      "---------------------------\n",
      "How do you create an Approval flow in Power Automate?\n",
      "---------------------------\n",
      "What is the \"Apply to each\" control?\n",
      "---------------------------\n",
      "How do you pass data between different steps in a flow?\n",
      "---------------------------\n",
      "What is the difference between an Automated and a Scheduled flow?\n",
      "---------------------------\n",
      "Explain the concept of parallel branches in a flow.\n",
      "---------------------------\n",
      "How can you handle large datasets in Power Automate?\n",
      "---------------------------\n",
      "What are HTTP actions, and when would you use them?\n",
      "---------------------------\n",
      "What is the Common Data Service (now Microsoft Dataverse)?\n",
      "---------------------------\n",
      "How would you secure a flow?\n",
      "---------------------------\n",
      "Explain the use of the Scope action.\n",
      "---------------------------\n",
      "What is RPA (Robotic Process Automation) and how is it used in Power Automate?\n",
      "---------------------------\n",
      "How do you connect Power Automate to on-premises data?\n",
      "---------------------------\n",
      "What are solutions in Power Automate?\n",
      "---------------------------\n",
      "How would you use a Do until loop?\n",
      "---------------------------\n",
      "What is AI Builder?\n",
      "---------------------------\n",
      "How can you use expressions to manipulate strings or dates?\n",
      "---------------------------\n",
      "Describe a flow you've built that uses a premium connector.\n",
      "---------------------------\n",
      "How do you share a flow with other users or a team?\n",
      "---------------------------\n",
      "How can you call a flow from a Power App?\n",
      "---------------------------\n",
      "What are some of the popular connectors you have worked with?\n",
      "---------------------------\n",
      "Scenario-Based Questions\n",
      "---------------------------\n",
      "Scenario: A manager wants an email with a summary of key metrics from a Power BI report every morning. How would you set this up?\n",
      "---------------------------\n",
      "Scenario: A sales rep needs to manually trigger an action from a Power BI report to send a personalized email to a customer with their latest sales data. How would you implement this?\n",
      "---------------------------\n",
      "Scenario: You have a Power BI report that shows customer feedback. You want to automatically create a new task in Planner for every \"Negative\" feedback entry. How would you build this?\n",
      "---------------------------\n",
      "Scenario: A data refresh for your Power BI dataset fails. How would you use Power Automate to send a notification to the responsible team?\n",
      "---------------------------\n",
      "Scenario: You need to allow users to export a filtered Power BI report to a SharePoint folder with the file name including the date. How would you achieve this?\n",
      "---------------------------\n",
      "Scenario: A user wants to click a button in a Power BI report to update a status in a SQL database table. How would you design this?\n",
      "---------------------------\n",
      "Scenario: You have a Power BI report with a list of products. A user wants to select a product and trigger a flow that updates a field in a Dynamics 365 record. How would you handle passing the selected product's ID to the flow?\n",
      "---------------------------\n",
      "Scenario: You need to automate the creation of a new user account in Active Directory whenever a new employee is added to a Power BI-generated report. How would you approach this?\n",
      "---------------------------\n",
      "Scenario: You have a large dataset in Excel Online. How would you create a flow to process each row and push the data to a Power BI streaming dataset for real-time visualization?\n",
      "---------------------------\n",
      "Scenario: A user wants to see a detailed, filtered view of a specific customer's data in a separate report page. How would you combine a Power BI button and Power Automate to achieve this?\n",
      "---------------------------\n",
      "Advanced & Niche Questions\n",
      "---------------------------\n",
      "Explain the process of creating a custom connector for an API that Power Automate doesn't have a built-in connector for.\n",
      "---------------------------\n",
      "How do you manage the deployment of Power Automate flows and Power BI reports across different environments (Dev, Test, Prod)?\n",
      "---------------------------\n",
      "What is the role of the Common Data Model (CDM) in the context of Power BI and Power Automate?\n",
      "---------------------------\n",
      "How would you handle a situation where a Power Automate flow needs to process more than 500,000 items from a SharePoint list for a Power BI report?\n",
      "---------------------------\n",
      "What are some best practices for managing permissions and licenses for Power BI and Power Automate users?\n",
      "---------------------------\n",
      "Explain how you would use Power Automate to integrate data from social media platforms into a Power BI report.\n",
      "---------------------------\n",
      "What are some key security considerations when building flows that interact with Power BI?\n",
      "---------------------------\n",
      "How would you use a child flow to modularize a complex process?\n",
      "---------------------------\n",
      "How does Power Automate Desktop differ from Power Automate for the web, and when would you use each for a Power BI task?\n",
      "---------------------------\n",
      "Explain how you would use HTTP Request triggers to create a custom API endpoint that a Power BI report can interact with.\n",
      "---------------------------\n",
      "What is the concept of a solution in the Power Platform, and why is it important for moving flows and apps?\n",
      "---------------------------\n",
      "How would you troubleshoot a Power Automate flow that is running slowly?\n",
      "---------------------------\n",
      "What is the purpose of the Terminate action?\n",
      "---------------------------\n",
      "How do you set up a Recurrence trigger?\n",
      "---------------------------\n",
      "What are some of the functions you use most frequently in Power Automate expressions?\n",
      "---------------------------\n",
      "How would you implement a simple approval process for a Power BI report?\n",
      "---------------------------\n",
      "What is Power BI Premium per User versus Premium Capacity?\n",
      "---------------------------\n",
      "What are the major differences between Power BI and Tableau?\n",
      "---------------------------\n",
      "What are the key features of the Power BI service?\n",
      "---------------------------\n",
      "How can Power Automate improve the data governance and compliance of your Power BI reports?\n",
      "---------------------------\n",
      "Describe a complex scenario where you used Power Automate and Power BI to solve a business problem from start to finish.\n",
      "---------------------------\n",
      "You can find more detailed answers to these questions in this video on Power Automate Tutorial: Mastering Scenario Based Interview Questions.\n",
      "---------------------------\n",
      "Power Automate can be integrated with Power BI to automate workflows and tasks, which greatly extends the functionality of Power BI beyond data visualization. This integration enables a variety of use cases, from simple report distribution to complex, data-driven actions.\n",
      "---------------------------\n",
      "Key Power Automate Use Cases with Power BI\n",
      "---------------------------\n",
      "Automating Report Distribution: This is one of the most common and valuable use cases. You can create a flow that automatically exports a Power BI report in a desired format (like PDF, PowerPoint, or Excel) and sends it to a list of stakeholders via email or Teams on a predefined schedule (e.g., daily, weekly, or monthly). This saves time and ensures everyone has the most up-to-date information without manual effort.\n",
      "---------------------------\n",
      "Triggering Actions from Reports: The Power Automate visual in Power BI lets users trigger a flow directly from a report. This can be used for various actions based on data context. For example, a user could click a button on a report to:\n",
      "---------------------------\n",
      "Send an email with a screenshot or specific data from the report.\n",
      "---------------------------\n",
      "Create a task in Planner or a ticket in a service management tool based on an overdue project or a support case.\n",
      "---------------------------\n",
      "Update a record in a SharePoint list or Dynamics 365.\n",
      "---------------------------\n",
      "Data-Driven Alerts and Notifications: You can set up flows to trigger when a data alert is met in the Power BI service. For example, if your sales figures drop below a certain threshold, a flow could automatically send a notification to the sales manager, create a Teams message for the team, or log the event in a SharePoint list for further action.\n",
      "---------------------------\n",
      "Automating Dataset Refreshes: While Power BI has its own scheduled refresh functionality, Power Automate provides more flexibility. You can use it to trigger a dataset refresh based on an event, such as a new file being uploaded to a SharePoint folder or a new row being added to an Excel spreadsheet. This ensures that your Power BI report is always displaying the latest data in near real-time.\n",
      "---------------------------\n",
      "Workflow Automation for Business Decisions: Power Automate can be used to initiate multi-step business processes based on Power BI data. For instance, a flow could be triggered when a report shows that a project budget is overspent, which then sends an approval request to a manager. Once approved, the flow could then send a notification to the finance department and update a financial record.\n",
      "---------------------------\n",
      "Using Power Automate to Refresh Power BI Datasets\n",
      "---------------------------\n",
      "This video demonstrates a very practical use case of Power Automate, showing you how to set up a flow to refresh a Power BI dataset automatically.\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredWordDocumentLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "\n",
    "MULTITHREADING = True\n",
    "MAXCONCURRENCY = 3\n",
    "UNSTRUCTURED_MODE = \"elements\"\n",
    "SPLITTERTHRSHD = 1000\n",
    "OVERLAP = 80\n",
    "tz = pytz.timezone('America/New_York')\n",
    "\n",
    "class WordDocProcessUnstructuredLoader:\n",
    "\n",
    "    def __init__(self, path : Path = \"\", fileFormat : str = \"docx\", chunkSize : str = SPLITTERTHRSHD, chunkOverlap : str = OVERLAP):\n",
    "\n",
    "        self.dirPath = path\n",
    "        self.fileFormat = fileFormat.lower()\n",
    "        self.loaderType = None\n",
    "        self.seperatorList = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        self.chunkSize = chunkSize\n",
    "        self.chunkOverlap = chunkOverlap\n",
    "        self.dirLoader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob= f\"**/*.{fileFormat.lower()}\",\n",
    "            loader_cls= self.loaderType,\n",
    "            loader_kwargs={\"mode\":UNSTRUCTURED_MODE},\n",
    "            show_progress=True,\n",
    "            use_multithreading= MULTITHREADING,\n",
    "            max_concurrency= MAXCONCURRENCY\n",
    "        )\n",
    "        self.textSplitter = RecursiveCharacterTextSplitter(\n",
    "                separators=self.seperatorList,\n",
    "                chunk_size = self.chunkSize,\n",
    "                chunk_overlap = self.chunkOverlap,\n",
    "                length_function = len\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def dirPath(self) -> Path:\n",
    "\n",
    "        return self.__dirPath\n",
    "    \n",
    "    @dirPath.setter\n",
    "    def dirPath(self, path : Path):\n",
    "\n",
    "        if not (path.exists() and str(path) != \"\" and str(path) != None ):\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "        elif str(path) == \"\" or str(path) == None:\n",
    "            raise ValueError(\"Path is Empty\")\n",
    "        else:\n",
    "            self.__dirPath = path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def fileFormat(self) -> str:\n",
    "        return self.__fileFormat\n",
    "    \n",
    "    @fileFormat.setter\n",
    "    def fileFormat(self, fileformat : str):\n",
    "\n",
    "        if fileformat.lower() == \"docx\":\n",
    "            self.__fileFormat = fileformat\n",
    "        else:\n",
    "            raise ValueError(\"File Format is other then DOCX\")\n",
    "        \n",
    "    @property\n",
    "    def loaderType(self):\n",
    "        return self.__loaderType\n",
    "    \n",
    "    @loaderType.setter\n",
    "    def loaderType(self, val):\n",
    "\n",
    "        if self.fileFormat.lower() == \"docx\":\n",
    "            self.__loaderType = UnstructuredWordDocumentLoader\n",
    "        else:\n",
    "            raise ValueError(\"File Type Other then DOCX\")\n",
    "        \n",
    "    @property\n",
    "    def chunkSize(self) -> str:\n",
    "        return self.__chunkSize\n",
    "    \n",
    "    @chunkSize.setter\n",
    "    def chunkSize(self, chunkSize : str) :\n",
    "        self.__chunkSize = chunkSize\n",
    "\n",
    "\n",
    "    @property\n",
    "    def chunkOverlap(self) -> str:\n",
    "        return self.__chunkOverlap\n",
    "    \n",
    "    @chunkOverlap.setter\n",
    "    def chunkOverlap(self, chunkOverlap : str):\n",
    "        self.__chunkOverlap = chunkOverlap \n",
    "\n",
    "        \n",
    "    @property\n",
    "    def loadDOCFromDir(self) -> list:\n",
    "        return self.dirLoader.load()\n",
    "    \n",
    "    def cleanText(self, text) -> str:\n",
    "\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    @property\n",
    "    def cleanChunks(self) -> list:\n",
    "\n",
    "        processed_chunks = []\n",
    "        elements = self.loadDOCFromDir\n",
    "\n",
    "        for page in elements:\n",
    "            cleanText = self.cleanText(page.page_content)\n",
    "            if len(cleanText) > SPLITTERTHRSHD:\n",
    "                chunks =self.textSplitter.create_documents(\n",
    "                    [cleanText],\n",
    "                    metadatas=[\n",
    "                        {\n",
    "                            **page.metadata,\n",
    "                            \"createddate\": datetime.now(tz=tz).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            \"updateddate\": datetime.now(tz=tz).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            \"createdby\" : \"kkhanvilkar\",\n",
    "                            \"updatedby\" : \"kkhanvilkar\"\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                processed_chunks.extend(chunks)\n",
    "                # print('process')\n",
    "            else:\n",
    "\n",
    "                processed_chunks.append(page)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "path = Path(\"../data\")\n",
    "\n",
    "obj = WordDocProcessUnstructuredLoader(path=path, fileFormat=\"docx\")\n",
    "elements = obj.cleanChunks\n",
    "for item in elements:\n",
    "    print(item.page_content)\n",
    "    print('---------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
